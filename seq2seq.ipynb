{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import scfg as cfg\n",
    "from utils.checkpoints import Checkpoints\n",
    "from asdl.parser import parse as parse_asdl\n",
    "from data.conala import ConalaDataset\n",
    "from model.copy_lstm import EncoderLSTM, DecoderLSTM, EncoderDecoder\n",
    "from main import calculate_errors, calculate_loss, train_epoch, evaluate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\", \"[SOA]\", \"[EOA]\"]\n",
    "\n",
    "# load Python ASDL grammar\n",
    "grammar = parse_asdl(\"src/asdl/Python.asdl\")\n",
    "\n",
    "# load CoNaLa intent-snippet pairs and map them to tensors\n",
    "# FIXME: train_ds and dev_ds are using different word and action mappings.\n",
    "# This causes dev performance to become completely nonsense.\n",
    "dataset_cache = cfg.model_dir / \"dataset_cache.pt\"\n",
    "if dataset_cache.exists():\n",
    "\ttrain_ds, dev_ds = torch.load(dataset_cache)\n",
    "\tprint(\"Loaded dataset cache\")\n",
    "else:\n",
    "\ttrain_ds = ConalaDataset(\n",
    "\t\t\"./data/conala-train.json\", grammar=grammar, special_tokens=special_tokens\n",
    "\t)\n",
    "\tdev_ds = ConalaDataset(\n",
    "\t\t\"./data/conala-dev.json\", grammar=grammar, special_tokens=special_tokens,\n",
    "\t\t action_vocab=train_ds.action_vocab, intent_vocab=train_ds.intent_vocab, shuffle=False\n",
    "\t)\n",
    "\ttorch.save((train_ds, dev_ds), dataset_cache)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded dataset cache\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "ids = train_ds.convert_intent_ids(\"Fastest Way to Drop Duplicated Index in a Pandas DataFrame\".lower())\n",
    "print(ids)\n",
    "tokens = train_ds.convert_ids_intent(ids)\n",
    "print(tokens)\n",
    "actions = train_ds.convert_ids_action([6, 7, 10, 5, 0])\n",
    "print(actions)\n",
    "ids = train_ds.convert_action_ids(actions)\n",
    "print(ids)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[829, 293, 14, 371, 544, 42, 9, 7, 43, 24]\n",
      "['fastest', 'way', 'to', 'drop', 'duplicated', 'index', 'in', 'a', 'pandas', 'dataframe']\n",
      "[('Reduce',), ('ApplyConstr', 'Load'), ('ApplyConstr', 'Constant'), '[EOA]', '[PAD]']\n",
      "[6, 7, 10, 5, 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# initialize the model and optimizer\n",
    "encoder = EncoderLSTM(\n",
    "\tvocab_size=train_ds.intent_vocab_size,\n",
    "\tdevice=cfg.device,\n",
    "\t**cfg.EncoderLSTM,\n",
    ")\n",
    "decoder = DecoderLSTM(\n",
    "\taction_size=train_ds.action_vocab_size,\n",
    "\tdevice=cfg.device,\n",
    "\t**cfg.DecoderLSTM,\n",
    ")\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def run_epoch(model, ds, optimizer, training=True):\n",
    "    model.train()\n",
    "    celoss = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    for batch in load(ds, training):\n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.transpose(1, 0)\n",
    "        labels = labels.transpose(1, 0)\n",
    "        input_mask = (inputs == 0)\n",
    "        label_mask = (labels == 0)\n",
    "        input_lens = (~input_mask).to(torch.long).sum(dim=1).cpu()\n",
    "        label_lens = (~label_mask).to(torch.long).sum(dim=1).cpu()\n",
    "        logits = model(inputs, labels, input_mask, label_mask, input_lens, label_lens)\n",
    "        act_vocab_l = logits.shape[-1]\n",
    "        loss = celoss(logits.reshape(-1, act_vocab_l), labels.reshape(-1))\n",
    "        losses.append(float(loss))\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return losses\n",
    "def load(ds, shuffle=True):\n",
    "    def collate_fn(data):\n",
    "        return (\n",
    "            torch.nn.utils.rnn.pad_sequence([input for input, _ in data]).to(cfg.device),\n",
    "            torch.nn.utils.rnn.pad_sequence([label for _, label in data]).to(cfg.device),\n",
    "        )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collate_fn\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for i in range(10):\n",
    "\tlosses = run_epoch(model, train_ds, optimizer)\n",
    "\tprint(f\"Training loss: {np.mean(losses)}\")\n",
    "\tlosses = run_epoch(model, dev_ds, optimizer, training=False)\n",
    "\tprint(f\"Valid loss: {np.mean(losses)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training loss: 0.8714710238600979\n",
      "Valid loss: 0.3985938012599945\n",
      "Training loss: 0.3149361566585653\n",
      "Valid loss: 0.2877434434990088\n",
      "Training loss: 0.2226134584355755\n",
      "Valid loss: 0.24412120406826338\n",
      "Training loss: 0.1758715825922349\n",
      "Valid loss: 0.2355222647388776\n",
      "Training loss: 0.14425291239964863\n",
      "Valid loss: 0.20250578299164773\n",
      "Training loss: 0.11360694370976016\n",
      "Valid loss: 0.14760115842024485\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('py39': conda)"
  },
  "interpreter": {
   "hash": "03b84ff73a0ac2061ea3f06c540e1588c4a0c99ec7f780bb902e545a8d82721e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}